use anyhow::{anyhow, Result};
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use serde_json::json;
use std::env;

/// A trait for Large Language Models (LLMs).
///
/// This trait defines the interface for a large language model, which is a
/// core component of the agent. The LLM is responsible for generating responses
/// based on a given prompt.
#[async_trait]
pub trait Llm {
    /// Takes a prompt and returns the LLM's response.
    ///
    /// # Arguments
    ///
    /// * `prompt` - A string representing the input to the LLM.
    ///
    /// # Returns
    ///
    /// A `Result` containing the LLM's response as a string, or an error if
    /// the call fails.
    async fn call(&self, prompt: &str) -> Result<String>;
}

/// An implementation of the `Llm` trait that connects to the OpenAI API.
///
/// This struct handles the communication with the OpenAI API, including
/// authentication, request formatting, and response parsing.
pub struct OpenAiLlm {
    client: Client,
    api_key: String,
    model: String,
}

impl OpenAiLlm {
    /// Creates a new `OpenAiLlm`.
    ///
    /// This function initializes the `reqwest` client and retrieves the OpenAI
    /// API key from the environment variables.
    ///
    /// # Returns
    ///
    /// A new instance of `OpenAiLlm`.
    pub fn new(model: &str) -> Self {
        let api_key = env::var("OPENAI_API_KEY").unwrap_or_else(|_| "".to_string());

        Self {
            client: Client::new(),
            api_key,
            model: model.to_string(),
        }
    }
}

#[async_trait]
impl Llm for OpenAiLlm {
    /// Sends a prompt to the OpenAI API and returns the response.
    ///
    /// This method constructs the API request, sends it to the OpenAI server,
    /// and parses the response to extract the content of the message.
    ///
    /// # Arguments
    ///
    /// * `prompt` - The prompt to send to the LLM.
    ///
    /// # Returns
    ///
    /// A `Result` containing the LLM's response, or an error if the request fails.
    async fn call(&self, prompt: &str) -> Result<String> {
        let response = self
            .client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&json!({
                "model": &self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that thinks step by step and provides your thoughts and actions in JSON format."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }))
            .send()
            .await?;

        if response.status().is_success() {
            let completion: Completion = response.json().await?;
            Ok(completion.choices[0].message.content.clone())
        } else {
            let error_body = response.text().await?;
            Err(anyhow!("API call failed: {}", error_body))
        }
    }
}

/// Represents the overall structure of the API response from OpenAI.
#[derive(Deserialize)]
struct Completion {
    choices: Vec<Choice>,
}

/// Represents a single "choice" or response generated by the LLM.
#[derive(Deserialize)]
struct Choice {
    message: Message,
}

/// Represents the message content from the LLM.
#[derive(Deserialize, Serialize)]
struct Message {
    role: String,
    content: String,
}

/// A mock implementation of the `Llm` trait for testing and demonstration.
///
/// `MockLlm` simulates the behavior of a real LLM by returning a canned response.
/// This allows for predictable testing of the agent's logic without making actual API calls.
pub struct MockLlm {
    response: String,
}

impl MockLlm {
    /// Creates a new `MockLlm` with a predefined response.
    pub fn new(response: &str) -> Self {
        Self {
            response: response.to_string(),
        }
    }
}

#[async_trait]
impl Llm for MockLlm {
    /// Simulates a call to an LLM by returning the predefined response.
    async fn call(&self, _prompt: &str) -> Result<String> {
        Ok(self.response.clone())
    }
}